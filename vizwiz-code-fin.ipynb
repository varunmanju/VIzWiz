{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom collections import Counter\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-31T17:55:55.64562Z","iopub.execute_input":"2022-03-31T17:55:55.646229Z","iopub.status.idle":"2022-03-31T17:55:55.651809Z","shell.execute_reply.started":"2022-03-31T17:55:55.646195Z","shell.execute_reply":"2022-03-31T17:55:55.65094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## resize the image\nfrom skimage import io\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize\n%matplotlib inline\n\ndef resize_image(image_url):\n    image = io.imread(image_url)\n    image = resize(image, (448,448))\n    return image\n  \n  ","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:55:56.23108Z","iopub.execute_input":"2022-03-31T17:55:56.231544Z","iopub.status.idle":"2022-03-31T17:55:56.239906Z","shell.execute_reply.started":"2022-03-31T17:55:56.231508Z","shell.execute_reply":"2022-03-31T17:55:56.237155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create json file for vocabulary\nimport json\nimport os\nimport nltk\nfrom nltk.stem.snowball import *\nfrom tqdm import *\nfrom collections import Counter, OrderedDict\nimport string\n\n\nannotation_dir = \"../input/vizwizvarun/Annotations/Annotations/\"\n\n\n## question\nq_counter = Counter()\nn_sample = 0\nmaxlen = 0\ntrain_data = json.load(open(annotation_dir + 'train.json'))\nfor one_data in tqdm(train_data):\n    n_sample += 1\n    question = one_data['question']\n    question = question.lower()\n    tokens = nltk.word_tokenize(question)\n    token_len = len(tokens)\n    maxlen = max([maxlen,token_len])\n    q_counter.update(tokens)\nprint('number of sample = ' + str(n_sample))\nprint('max len = ' + str(maxlen))\nq_word_counts = [x for x in q_counter.items()]\nq_word_counts.sort(key=lambda x: x[1], reverse=True)\njson.dump(q_word_counts, open('q_word_counts.json', \"w\"), indent=2)\n\n### build vocabulary based on question\nvocab = [x[0] for x in q_word_counts if x[1] >= 0]\nunk_word = '<UNK>'\nvocab = [unk_word] + vocab\nvocab = OrderedDict(zip(vocab,range(len(vocab))))\njson.dump(vocab, open('word2vocab_id.json', 'w'), indent=2)\n\n## answer\nans_counter = Counter()\n\ntrain_data = json.load(open(annotation_dir + 'train.json'))\nfor annotation in tqdm(train_data):\n    for answer in annotation['answers']:\n        answer = answer['answer'].lower()\n        ans_counter.update([answer]) # don't forget the [], counter.update input a list\nans_counts = [x for x in ans_counter.items()]\nans_counts.sort(key=lambda x: x[1], reverse=True)\njson.dump(ans_counts, open('ans_counts.json', \"w\"), indent=2)\n\n### build answer candidates\noutput_num = 3000\nn_totoal = sum([x[1] for x in ans_counts])\nans_counts = ans_counts[:output_num]\nn_cover = sum([x[1] for x in ans_counts])\nprint(\"we keep top %d most answers\"%len(ans_counts))\nprint(\"coverage: %d/%d (%.4f)\"%(n_cover, n_totoal, 1.0 * n_cover / n_totoal))\nans_list = [x[0] for x in ans_counts]\nans_list.append('<unk>')\nans_dict = OrderedDict(zip(ans_list,range(len(ans_list))))\njson.dump(ans_dict, open('answer2answer_id.json', 'w'), indent=2)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:56:01.906355Z","iopub.execute_input":"2022-03-31T17:56:01.906927Z","iopub.status.idle":"2022-03-31T17:56:07.568155Z","shell.execute_reply.started":"2022-03-31T17:56:01.906868Z","shell.execute_reply":"2022-03-31T17:56:07.567381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qvocabsize=0\ndata=json.load(open(\"./q_word_counts.json\"))\nfor i in data:\n    qvocabsize+=1","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:56:11.856594Z","iopub.execute_input":"2022-03-31T17:56:11.856858Z","iopub.status.idle":"2022-03-31T17:56:11.86486Z","shell.execute_reply.started":"2022-03-31T17:56:11.856826Z","shell.execute_reply":"2022-03-31T17:56:11.864091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qvocabsize","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:56:12.337275Z","iopub.execute_input":"2022-03-31T17:56:12.337933Z","iopub.status.idle":"2022-03-31T17:56:12.345763Z","shell.execute_reply.started":"2022-03-31T17:56:12.337872Z","shell.execute_reply":"2022-03-31T17:56:12.344938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Resize the images to 448*448 and save the training images\nfrom PIL import Image\nfrom tqdm import *\nimport numpy as np\nfrom skimage.io import imsave, imread\nimport os\nimport json\nimport sys\n\n\n  \nimage_size = 448\n\n\nmain_dir = \"../input/vizwizvarun\"\ntrain_dataset = main_dir + '/train/train'\noutput=\"images_trainedresized\"\n\nimages_name = os.listdir(train_dataset)\nimages_name.sort()\nprint(len(images_name))\nif not os.path.exists(output):\n    os.makedirs(output) \nfor idx, img_info in enumerate(tqdm(images_name)):\n    \n    img_path = train_dataset+\"/\"+img_info\n    \n    image = resize_image(img_path)\n    imsave(os.path.join(output, img_info),image)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:22:12.622758Z","iopub.execute_input":"2022-03-27T18:22:12.623416Z","iopub.status.idle":"2022-03-27T18:22:15.803173Z","shell.execute_reply.started":"2022-03-27T18:22:12.623382Z","shell.execute_reply":"2022-03-27T18:22:15.801992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Resize the images to 448*448 and save the validation images\nfrom tqdm import *\nimport numpy as np\nimport os\nimport json\nfrom PIL import Image\nimport sys\nfrom skimage.io import imsave, imread\n\n  \nimage_size = 448\n\nmain_dir = \"../input/vizwizvarun\"\noutput=\"images_resized_val\"\ndataset_dir = main_dir + '/val/val'\nimages_name = os.listdir(dataset_dir)\nimages_name.sort()\nprint(len(images_name))\nif not os.path.exists(output):\n    os.makedirs(output) \nfor idx, img_info in enumerate(tqdm(images_name)):\n    img_path = dataset_dir+\"/\"+img_info\n    image = resize_image(img_path)\n    imsave(os.path.join(output, img_info),image)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T18:21:58.802588Z","iopub.status.idle":"2022-03-27T18:21:58.803014Z","shell.execute_reply.started":"2022-03-27T18:21:58.802777Z","shell.execute_reply":"2022-03-27T18:21:58.802801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import *\nimport numpy as np\nimport os\nimport json\nfrom PIL import Image\n\n\n\n\nimport sys\nfrom skimage.io import imsave, imread\n\n  \nimage_size = 448\n\nmain_dir = \"../input/vizwizvarun\"\n\n\n\noutput=\"images_testresized\"\ndataset_dir = main_dir + '/test/test'\nall_imgs = os.listdir(dataset_dir)\nall_imgs.sort()\nprint(len(all_imgs))\nif not os.path.exists(output):\n    os.makedirs(output) \nfor idx, img_info in enumerate(tqdm(all_imgs[0:1000])):\n    \n    img_path = dataset_dir+\"/\"+img_info\n    \n    image = resize_image(img_path)\n    imsave(os.path.join(output, img_info),image)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:10:46.098276Z","iopub.execute_input":"2022-03-31T18:10:46.09854Z","iopub.status.idle":"2022-03-31T18:14:37.338593Z","shell.execute_reply.started":"2022-03-31T18:10:46.098509Z","shell.execute_reply":"2022-03-31T18:14:37.337916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport numpy as np\nimport nltk\nimport os\nfrom tqdm import *\nfrom collections import Counter\nimport re\noutput_dir=\"data\"\n\ndef convert_to_tokens(sentence):\n\n    regex = re.compile(r'(\\W+)')\n    tokens = regex.split(sentence.lower())\n    tokens = [w.strip() for w in tokens if len(w.strip()) > 0]\n    return tokens\n\nroot_dir = \"../input/vizwizvarun/Annotations/Annotations\"\n#splits = ['train', 'val', 'test']\n\ndirs=\"../input/vizwizvarun/\"\nvocab = json.load(open('./word2vocab_id.json'))\nanswer2answer_id = json.load(open('./answer2answer_id.json'))\n#../input/vizwizvarun/train/train/VizWiz_train_00000000.jpg\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:57:05.835651Z","iopub.execute_input":"2022-03-31T17:57:05.836256Z","iopub.status.idle":"2022-03-31T17:57:05.847605Z","shell.execute_reply.started":"2022-03-31T17:57:05.836216Z","shell.execute_reply":"2022-03-31T17:57:05.846868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentence_to_vocabid(sentence, vocab):\n    unk_word = '<UNK>'\n    tokens = nltk.word_tokenize(sentence.lower())\n    tokens_id = [vocab.get(x, vocab[unk_word]) for x in tokens]\n    return tokens_id","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:57:06.412286Z","iopub.execute_input":"2022-03-31T17:57:06.412838Z","iopub.status.idle":"2022-03-31T17:57:06.418748Z","shell.execute_reply.started":"2022-03-31T17:57:06.412799Z","shell.execute_reply":"2022-03-31T17:57:06.418006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessingtrain(type1):\n        missing_images = []\n\n   \n    ## load annotation file and question file\n        dataset = json.load(open(root_dir+'/'+type1+'.json'))\n        data = [None]*len(dataset)\n\n        for idx,one_data in enumerate(dataset):\n            ans_counter = Counter([x['answer'] for x in one_data['answers']])\n            allanswers = [x['answer'] for x in one_data['answers']]\n            ans = ans_counter.most_common(1)[0][0]\n            a_label = answer2answer_id.get(ans,3000)\n            answers=[]\n#             if type1 == 'train' and a_label == -1:\n#                 continue\n            for answer in allanswers :\n                answers.append(answer2answer_id.get(answer,3000))\n            \n       \n            imagename = one_data['image']\n            imagepath = \"../input/trainresized/images_trainedresized\" \"/\"+ imagename\n            questionsentence=one_data['question']\n            questiontokens = nltk.word_tokenize(question)\n            questionencoding = sentence_to_vocabid(one_data['question'], vocab)\n            if(len(questionencoding)>=37):\n                questionencoding=questionencoding[0:37]\n            else:\n                questionencoding.extend([0]*(37-len(questionencoding)))\n            image_info = {'imagename': imagename,\n                'imagepath': imagepath,\n                'questionsentence': questionsentence,\n                'questiontokens': questiontokens\n                }\n            \n            image_info['questionencoding'] = questionencoding\n            image_info['selectedanswer'] = ans\n            image_info['answerlabel']=a_label\n            image_info['answers']=answers\n            data[idx]=image_info\n        return data","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:57:01.142153Z","iopub.execute_input":"2022-03-31T17:57:01.142912Z","iopub.status.idle":"2022-03-31T17:57:01.154312Z","shell.execute_reply.started":"2022-03-31T17:57:01.142848Z","shell.execute_reply":"2022-03-31T17:57:01.153496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessingvalidation(type1):\n        missing_images = []\n\n   \n    ## load annotation file and question file\n        dataset = json.load(open(root_dir+'/'+type1+'.json'))\n        data = [None]*len(dataset)\n\n        for idx,one_data in enumerate(dataset):\n            ans_counter = Counter([x['answer'] for x in one_data['answers']])\n            allanswers = [x['answer'] for x in one_data['answers']]\n            ans = ans_counter.most_common(1)[0][0]\n            a_label = answer2answer_id.get(ans,3000)\n            answers=[]\n#             if type1 == 'val' and a_label == -1:\n#                 continue\n            for answer in allanswers :\n                answers.append(answer2answer_id.get(answer,3000))\n            \n       \n            imagename = one_data['image']\n            imagepath = \"../input/validationresized/images_resized_val\" \"/\"+ imagename\n            questionsentence=one_data['question']\n            questiontokens = nltk.word_tokenize(question)\n            questionencoding = sentence_to_vocabid(one_data['question'], vocab)\n            if(len(questionencoding)>=37):\n                questionencoding=questionencoding[0:37]\n            else:\n                questionencoding.extend([0]*(37-len(questionencoding)))\n            \n            image_info = {'imagename': imagename,\n                'imagepath': imagepath,\n                'questionsentence': questionsentence,\n                'questiontokens': questiontokens\n                         }\n            \n            image_info['questionencoding'] = questionencoding\n            image_info['selectedanswer'] = ans\n            image_info['answerlabel']=a_label\n            image_info['answers']=answers\n            data[idx]=image_info\n        return data","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:57:01.368005Z","iopub.execute_input":"2022-03-31T17:57:01.368694Z","iopub.status.idle":"2022-03-31T17:57:01.378902Z","shell.execute_reply.started":"2022-03-31T17:57:01.368652Z","shell.execute_reply":"2022-03-31T17:57:01.378144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sampledata(data,num):\n    import random\n    random.shuffle(data)\n    return data[0:num]","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:57:02.625402Z","iopub.execute_input":"2022-03-31T17:57:02.625877Z","iopub.status.idle":"2022-03-31T17:57:02.629659Z","shell.execute_reply.started":"2022-03-31T17:57:02.625843Z","shell.execute_reply":"2022-03-31T17:57:02.628953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprecessed={}\npreprecessed['train']=sampledata(preprocessingtrain('train'),10000)\npreprecessed['val']=sampledata(preprocessingvalidation('val'),1000)\npreprecessed['train-val'] = preprecessed['train'] + preprecessed['val']\n","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:57:09.688259Z","iopub.execute_input":"2022-03-31T17:57:09.689069Z","iopub.status.idle":"2022-03-31T17:57:17.271837Z","shell.execute_reply.started":"2022-03-31T17:57:09.689016Z","shell.execute_reply":"2022-03-31T17:57:17.271046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(preprecessed['val'])","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:25:20.904165Z","iopub.execute_input":"2022-03-31T17:25:20.904498Z","iopub.status.idle":"2022-03-31T17:25:20.911798Z","shell.execute_reply.started":"2022-03-31T17:25:20.904462Z","shell.execute_reply":"2022-03-31T17:25:20.911138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs(\"data\")","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:57:21.066108Z","iopub.execute_input":"2022-03-31T17:57:21.06668Z","iopub.status.idle":"2022-03-31T17:57:21.071852Z","shell.execute_reply.started":"2022-03-31T17:57:21.06664Z","shell.execute_reply":"2022-03-31T17:57:21.070592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" for key, value in preprecessed.items():\n        np.save(os.path.join(\"./data\", f'{key}.npy'), np.array(value))","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:25:20.936803Z","iopub.status.idle":"2022-03-31T17:25:20.937154Z","shell.execute_reply.started":"2022-03-31T17:25:20.936975Z","shell.execute_reply":"2022-03-31T17:25:20.936998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os\n\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\n\nINPUT_DIR = './data'\n\nclass VizWizdataset(Dataset):\n\n    def __init__(self, input_dir, file, transformimage = None):\n\n        self.input_data = np.load(os.path.join(input_dir, file), allow_pickle=True)\n        self.nottest = True if not \"test\" in file else False\n        self.transformimage = transformimage\n\n    def __getitem__(self, idx):\n        if(self.input_data[idx] is None): \n            path = self.input_data[1]['imagepath']\n            img = np.array(Image.open(path))\n            questiontokens = self.input_data[1]['questiontokens']\n            questiontoid=self.input_data[1]['questionencoding']\n            if(len(questiontoid)>=37):\n                questiontoid=questiontoid[0:37]\n            else:\n                questiontoid=questiontoid.extend([0]*(37-len(questiontoid)))\n            questiontoid=(np.asarray(self.input_data[1]['questionencoding']))\n            \n            dataentry = {'image': img, 'question': questiontoid}\n            if(self.nottest):\n                dataentry['answers']=np.array(self.input_data[1]['answers'])\n            if self.transformimage:\n                dataentry['image'] = self.transformimage(dataentry['image'])\n                dataentry['imagepath']=self.input_data[1]['imagepath']\n            if self.nottest:\n                answertoid = self.input_data[1]['answerlabel']\n                dataentry['answer'] = answertoid\n            return dataentry\n        else:\n            path = self.input_data[idx]['imagepath']\n            #print(path)\n            img = np.array(Image.open(path))\n            questiontokens = self.input_data[idx]['questiontokens']\n            questiontoid=self.input_data[idx]['questionencoding']\n            if(len(questiontoid)>=37):\n                questiontoid=questiontoid[0:37]\n            else:\n                questiontoid=questiontoid.extend([0]*(37-len(questiontoid)))\n            questiontoid=(np.asarray(self.input_data[idx]['questionencoding']))\n            dataentry = {'image': img, 'question': questiontoid}\n            if(self.nottest):\n                dataentry['answers']=np.array(self.input_data[idx]['answers'])\n            if self.nottest:\n                answertoid = self.input_data[idx]['answerlabel']\n                dataentry['answer'] = answertoid\n\n            if self.transformimage:\n                dataentry['image'] = self.transformimage(dataentry['image'])\n                dataentry['imagepath']=self.input_data[idx]['imagepath']\n\n            return dataentry\n\n    def __len__(self):\n\n        return len(self.input_data)\n\ndef data_loader(input_dir, batch_size, num_worker):\n\n    transformimage = transforms.Compose([\n        transforms.ToTensor(),  # convert to a tensor\n        transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    )                          # normalize the image so pizel values lie between 0 and 1\n    ])\n\n    vizwiz = {\n        'train': VizWizdataset(\n            input_dir=input_dir,\n            file='train.npy',\n            transformimage=transformimage),\n        'val': VizWizdataset(\n            input_dir=input_dir,\n            file='val.npy',\n            transformimage=transformimage)\n    }\n\n    dataloader = {\n        'train': DataLoader(\n            dataset=vizwiz['train'],\n            batch_size=batch_size,\n            shuffle=True,\n            num_workers=num_worker\n            ),\n        'val': DataLoader(\n            dataset=vizwiz['val'],\n            batch_size=batch_size,\n            shuffle=True,\n            num_workers=num_worker\n            )\n        \n    }\n\n    return dataloader\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:24:05.805536Z","iopub.execute_input":"2022-03-31T18:24:05.805834Z","iopub.status.idle":"2022-03-31T18:24:05.827113Z","shell.execute_reply.started":"2022-03-31T18:24:05.805799Z","shell.execute_reply":"2022-03-31T18:24:05.826128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.nn.functional as F\n\n\nclass ImgPreprocess(nn.Module):\n\n    def __init__(self, embed_dim):\n\n        super(ImgPreprocess, self).__init__()\n        self.model = models.resnet50(pretrained=True)\n        in_features = self.model.fc.in_features\n        self.model.fc =  nn.Linear(in_features, embed_dim)# remove resnet50 last layer\n      \n\n    def forward(self, image):\n        with torch.no_grad():\n            imagefeatures = self.model(image) # (batch, channel, height, width)\n            \n        l2_norm = F.normalize(imagefeatures, p=2, dim=1).detach()\n        return l2_norm\n\nclass QuestionPreprocess(nn.Module):\n    def __init__(self, questionvocabularysize, wordembeddingsize, hidden_size, num_hidden, sizeofconcatenatedlayer):\n                super(QuestionPreprocess, self).__init__()\n                self.word_embedding = nn.Embedding(questionvocabularysize, wordembeddingsize)\n                self.tanh = nn.Tanh()\n                self.hidden_size=hidden_size\n                self.num_hidden=num_hidden\n                self.lstm=nn.LSTM(wordembeddingsize,hidden_size,num_hidden,batch_first=True,bidirectional=True)\n                self.fc=nn.Linear(hidden_size*2,sizeofconcatenatedlayer)\n    def forward(self, question):\n                qu_embedding = self.word_embedding(question)  \n                qu_embedding = self.tanh(qu_embedding)\n                h0=torch.zeros(self.num_hidden*2,qu_embedding.size(0),self.hidden_size).to('cuda')\n                c0=torch.zeros(self.num_hidden*2,qu_embedding.size(0),self.hidden_size).to('cuda')\n                output,_=self.lstm(qu_embedding,(h0,c0))\n                output=self.fc(output[:,-1,:])\n                return output\nclass VizWizModel(nn.Module):\n\n    def __init__(self, sizeoffeatures, questionvocabularysize, answervocabularysize, wordembeddingsize, hidden_size, num_hidden):\n\n        super(VizWizModel, self).__init__()\n        self.questionpreprocess = QuestionPreprocess(questionvocabularysize, wordembeddingsize, hidden_size, num_hidden, sizeoffeatures)\n        self.imgpreprocess = ImgPreprocess(sizeoffeatures)\n        self.tanh = nn.Tanh()\n        self.dropout = nn.Dropout(0.5)\n        self.layer1 = nn.Linear(sizeoffeatures, answervocabularysize)\n        self.layer2 = nn.Linear(answervocabularysize, answervocabularysize)\n\n    def forward(self, image, question):\n\n        imagefeatures = self.imgpreprocess(image)               \n        queationfeatures = self.questionpreprocess(question)\n        dotproductfeatures = imagefeatures * queationfeatures\n        dotproductfeatures = self.dropout(dotproductfeatures)\n        dotproductfeatures = self.tanh(dotproductfeatures)\n        dotproductfeatures = self.layer1(dotproductfeatures)       \n        dotproductfeatures = self.dropout(dotproductfeatures)\n        dotproductfeatures = self.tanh(dotproductfeatures)\n        logits = self.layer2(dotproductfeatures)                 \n\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:57:28.629561Z","iopub.execute_input":"2022-03-31T17:57:28.629813Z","iopub.status.idle":"2022-03-31T17:57:28.63634Z","shell.execute_reply.started":"2022-03-31T17:57:28.629782Z","shell.execute_reply":"2022-03-31T17:57:28.635583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport torch.nn as nn\n\nfrom torch import optim\n\n\ntrain_accuracy=[]\nvalid_accuracy=[]\nepochs_total=[]\ndatadir = './data'\nmodel_dir = './ckpt'\nlogs = './log'\n\nworkers = 1\nbatchsize = 100\nsizeoffeatures, embeddingsize = 1024, 300\nnumofhidden, sizeofhiddenlayer = 3, 512\n\nepochs = 10\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#device='cuda'\nprint(device)\ndef train():\n    dataloader = data_loader(input_dir=datadir, batch_size=batchsize, num_worker=workers)\n    questionvocabularysize = qvocabsize+1\n    answervocabularysize = 3001\n    \n    model = VizWizModel(sizeoffeatures=sizeoffeatures, questionvocabularysize=questionvocabularysize, answervocabularysize=answervocabularysize,wordembeddingsize=embeddingsize, hidden_size=sizeofhiddenlayer, num_hidden=numofhidden).to(device)\n    \n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n   \n   \n    print('>> start training')\n    start_time = time.time()\n    for epoch in range(epochs):\n        metrics=[]\n        metrics1=[]\n        correct1=0\n        length1=0\n        correct=0\n        length=0\n        epoch_loss = {key: 0 for key in ['train', 'val']}\n        print(epoch)\n        model.train()\n        print(dataloader['train'])\n        for idx, dataentry in enumerate(dataloader['train']):\n            \n            image = dataentry['image'].to(device)\n            question = dataentry['question'].to(device)\n            label = dataentry['answer'].to(device)\n            answers=dataentry['answers']\n            label1=label.tolist()\n            \n            # forwardpropogation\n            logits = model(image, question)\n            \n            with torch.no_grad():\n                predict = torch.log_softmax(logits, dim=1)\n                predict = torch.argmax(predict, dim=1).tolist()\n            print(\"Train set prediction {}\".format(Counter(predict)))\n            for i in range(len(label1)):\n                if(predict[i]==label1[i]):\n                    correct+=1\n            length=length+batchsize\n            for i in range(answers.shape[0]):\n                count=0\n                for j in range(answers.shape[1]):\n                    if(answers[i][j]==predict[i]):\n                        count+=1\n                metrics.append(min(count//3,1))\n            loss = criterion(logits, label)\n            epoch_loss['train'] += loss.item()\n            # backpropogation\n            optimizer.zero_grad()\n            loss.backward()\n            print(loss.item())\n            optimizer.step()\n        model.eval()\n        correct = correct/length \n        print(\"Train set Accuracy is {}\".format(correct))\n        print(\"Train set Accuracy metric is {}\".format(np.mean(metrics)))\n        train_accuracy.append(np.mean(metrics))\n        \n        for idx, dataentry in enumerate(dataloader['val']):\n            image = dataentry['image'].to(device=device)\n            question = dataentry['question'].to(device=device)\n            label = dataentry['answer'].to(device=device)\n            answers=dataentry['answers']\n            label1=label\n            with torch.no_grad():\n                logits = model(image, question)\n                loss = criterion(logits, label)\n            epoch_loss['val'] += loss.item()\n            with torch.no_grad():\n                predict = torch.log_softmax(logits, dim=1)\n                predict = torch.argmax(predict, dim=1).tolist()\n            print(\"Valid set prediction {}\".format(Counter(predict)))\n            for i in range(len(predict)):\n                if(predict[i]==label1[i]):\n                    correct1+=1\n            length1=length1+batchsize\n            for i in range(answers.shape[0]):\n                count=0\n                for j in range(answers.shape[1]):\n                    if(answers[i][j]==predict[i]):\n                        count+=1\n                metrics1.append(min(count//3,1))\n        correct1 = correct1/length1\n        print(\"Validation set Accuracy is {}\".format(correct1))\n        print(\"Validation set Accuracy metric is {}\".format(np.mean(metrics1)))\n        valid_accuracy.append(np.mean(metrics1))\n        epochs_total.append(epoch)\n        # statistic\n        for phase in ['train', 'val']:\n            epoch_loss[phase] /= len(dataloader[phase])\n            with open(os.path.join(logs, f'{phase}_log.txt'), 'a') as f:\n                f.write(str(epoch+1) + '\\t' + str(epoch_loss[phase]) + '\\n')\n        print('Epoch:{}/{} | Training Loss: {train:6f} | Validation Loss: {val:6f}'.format(epoch+1, epochs, **epoch_loss))\n          \n        torch.save(model.state_dict(), os.path.join(model_dir, f'model-epoch-{epoch+1}.pth'))\n\n        scheduler.step()\n    end_time = time.time()\n    training_time = end_time - start_time\n    print(\"Total training time in mins is {}\".format(training_time//60))\n    \nif not os.path.exists(logs):\n    os.makedirs(logs)\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\ntrain()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:57:31.500228Z","iopub.execute_input":"2022-03-31T17:57:31.500792Z","iopub.status.idle":"2022-03-31T17:57:31.511536Z","shell.execute_reply.started":"2022-03-31T17:57:31.500754Z","shell.execute_reply":"2022-03-31T17:57:31.510776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# importing package\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# plot lines\nplt.plot(epochs_total, train_accuracy, label = \"train_accuracy\")\nplt.plot(epochs_total, valid_accuracy, label = \"validation_accuracy\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:57:32.118782Z","iopub.execute_input":"2022-03-31T17:57:32.119063Z","iopub.status.idle":"2022-03-31T17:57:32.125505Z","shell.execute_reply.started":"2022-03-31T17:57:32.119032Z","shell.execute_reply":"2022-03-31T17:57:32.124706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport torch.nn as nn\n\nfrom torch import optim\n\n\ntrain_accuracy=[]\nvalid_accuracy=[]\nepochs_total=[]\ndatadir = './data'\nmodel_dir = './ckpt'\nlogs = './log'\n\nworkers = 1\nbatchsize = 100\nsizeoffeatures, embeddingsize = 1024, 1\nsizeoffeatures, embeddingsize = 1024, 150\nnumofhidden, sizeofhiddenlayer = 2, 256\n\nepochs = 10\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#device='cuda'\nprint(device)\ndef train():\n    dataloader = data_loader(input_dir=datadir, batch_size=batchsize, num_worker=workers)\n    questionvocabularysize = qvocabsize+1\n    answervocabularysize = 3001\n    \n    model = VizWizModel(sizeoffeatures=sizeoffeatures, questionvocabularysize=questionvocabularysize, answervocabularysize=answervocabularysize,wordembeddingsize=embeddingsize, hidden_size=sizeofhiddenlayer, num_hidden=numofhidden).to(device)\n    \n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n   \n   \n    print('>> start training')\n    start_time = time.time()\n    for epoch in range(epochs):\n        metrics=[]\n        metrics1=[]\n        correct1=0\n        length1=0\n        correct=0\n        length=0\n        epoch_loss = {key: 0 for key in ['train', 'val']}\n        print(epoch)\n        model.train()\n        print(dataloader['train'])\n        for idx, dataentry in enumerate(dataloader['train']):\n            \n            image = dataentry['image'].to(device)\n            question = dataentry['question'].to(device)\n            label = dataentry['answer'].to(device)\n            answers=dataentry['answers']\n            label1=label.tolist()\n            \n            # forwardpropogation\n            logits = model(image, question)\n            \n            with torch.no_grad():\n                predict = torch.log_softmax(logits, dim=1)\n                predict = torch.argmax(predict, dim=1).tolist()\n            print(\"Train set prediction {}\".format(Counter(predict)))\n            for i in range(len(label1)):\n                if(predict[i]==label1[i]):\n                    correct+=1\n            length=length+batchsize\n            for i in range(answers.shape[0]):\n                count=0\n                for j in range(answers.shape[1]):\n                    if(answers[i][j]==predict[i]):\n                        count+=1\n                metrics.append(min(count//3,1))\n            loss = criterion(logits, label)\n            epoch_loss['train'] += loss.item()\n            # backpropogation\n            optimizer.zero_grad()\n            loss.backward()\n            print(loss.item())\n            optimizer.step()\n        model.eval()\n        correct = correct/length \n        print(\"Train set Accuracy is {}\".format(correct))\n        print(\"Train set Accuracy metric is {}\".format(np.mean(metrics)))\n        train_accuracy.append(np.mean(metrics))\n        \n        for idx, dataentry in enumerate(dataloader['val']):\n            image = dataentry['image'].to(device=device)\n            question = dataentry['question'].to(device=device)\n            label = dataentry['answer'].to(device=device)\n            answers=dataentry['answers']\n            label1=label\n            with torch.no_grad():\n                logits = model(image, question)\n                loss = criterion(logits, label)\n            epoch_loss['val'] += loss.item()\n            with torch.no_grad():\n                predict = torch.log_softmax(logits, dim=1)\n                predict = torch.argmax(predict, dim=1).tolist()\n            print(\"Valid set prediction {}\".format(Counter(predict)))\n            for i in range(len(predict)):\n                if(predict[i]==label1[i]):\n                    correct1+=1\n            length1=length1+batchsize\n            for i in range(answers.shape[0]):\n                count=0\n                for j in range(answers.shape[1]):\n                    if(answers[i][j]==predict[i]):\n                        count+=1\n                metrics1.append(min(count//3,1))\n        correct1 = correct1/length1\n        print(\"Validation set Accuracy is {}\".format(correct1))\n        print(\"Validation set Accuracy metric is {}\".format(np.mean(metrics1)))\n        valid_accuracy.append(np.mean(metrics1))\n        epochs_total.append(epoch)\n        # statistic\n        for phase in ['train', 'val']:\n            epoch_loss[phase] /= len(dataloader[phase])\n            with open(os.path.join(logs, f'{phase}_log.txt'), 'a') as f:\n                f.write(str(epoch+1) + '\\t' + str(epoch_loss[phase]) + '\\n')\n        print('Epoch:{}/{} | Training Loss: {train:6f} | Validation Loss: {val:6f}'.format(epoch+1, epochs, **epoch_loss))\n          \n        torch.save(model.state_dict(), os.path.join(model_dir, f'model-epoch-{epoch+1}.pth'))\n\n        scheduler.step()\n    end_time = time.time()\n    training_time = end_time - start_time\n    print(\"Total training time in mins is {}\".format(training_time//60))\n    \nif not os.path.exists(logs):\n    os.makedirs(logs)\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\ntrain()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:25:59.669975Z","iopub.execute_input":"2022-03-31T17:25:59.670295Z","iopub.status.idle":"2022-03-31T17:25:59.67825Z","shell.execute_reply.started":"2022-03-31T17:25:59.670261Z","shell.execute_reply":"2022-03-31T17:25:59.677435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# importing package\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# plot lines\nplt.plot(epochs_total, train_accuracy, label = \"train_accuracy\")\nplt.plot(epochs_total, valid_accuracy, label = \"validation_accuracy\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:26:00.124783Z","iopub.execute_input":"2022-03-31T17:26:00.125277Z","iopub.status.idle":"2022-03-31T17:26:00.129342Z","shell.execute_reply.started":"2022-03-31T17:26:00.125238Z","shell.execute_reply":"2022-03-31T17:26:00.128638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.nn.functional as F\n\n\nclass ImgPreprocess(nn.Module):\n\n    def __init__(self, embed_dim):\n\n        super(ImgPreprocess, self).__init__()\n        self.model = models.vgg19(pretrained=True)\n        in_features = self.model.classifier[-1].in_features\n        self.model.classifier[-1] =  nn.Linear(in_features, embed_dim)# remove resnet50 last layer\n      \n    def forward(self, image):\n\n        with torch.no_grad():\n           \n            imagefeatures = self.model(image) # (batch, channel, height, width)\n        l2_norm = F.normalize(imagefeatures, p=2, dim=1).detach()\n        return l2_norm\n\nclass QuestionPreprocess(nn.Module):\n\n    def __init__(self, questionvocabularysize, wordembeddingsize, hidden_size, num_hidden, sizeofconcatenatedlayer):\n                super(QuestionPreprocess, self).__init__()\n                self.word_embedding = nn.Embedding(questionvocabularysize, wordembeddingsize)\n                self.tanh = nn.Tanh()\n                self.hidden_size=hidden_size\n                self.num_hidden=num_hidden\n                self.lstm=nn.LSTM(wordembeddingsize,hidden_size,num_hidden,batch_first=True,bidirectional=True)\n                self.fc=nn.Linear(hidden_size*2,sizeofconcatenatedlayer)\n    def forward(self, question):\n                qu_embedding = self.word_embedding(question)  \n                qu_embedding = self.tanh(qu_embedding)\n                output,_=self.lstm(qu_embedding)\n                output=self.fc(output[:,-1,:])\n                return output\nclass VizWizModel(nn.Module):\n\n    def __init__(self, sizeoffeatures, questionvocabularysize, answervocabularysize, wordembeddingsize, hidden_size, num_hidden):\n\n        super(VizWizModel, self).__init__()\n        self.questionpreprocess = QuestionPreprocess(questionvocabularysize, wordembeddingsize, hidden_size, num_hidden, sizeoffeatures)\n        self.imgpreprocess = ImgPreprocess(sizeoffeatures)\n        self.tanh = nn.Tanh()\n        self.dropout = nn.Dropout(0.5)\n        self.layer1 = nn.Linear(sizeoffeatures, answervocabularysize)\n        self.layer2 = nn.Linear(answervocabularysize, answervocabularysize)\n\n    def forward(self, image, question):\n\n        imagefeatures = self.imgpreprocess(image)               \n        queationfeatures = self.questionpreprocess(question)\n        dotproductfeatures = imagefeatures * queationfeatures\n        dotproductfeatures = self.dropout(dotproductfeatures)\n        dotproductfeatures = self.tanh(dotproductfeatures)\n        dotproductfeatures = self.layer1(dotproductfeatures)       \n        dotproductfeatures = self.dropout(dotproductfeatures)\n        dotproductfeatures = self.tanh(dotproductfeatures)\n        logits = self.layer2(dotproductfeatures)                 \n\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:26:00.618988Z","iopub.execute_input":"2022-03-31T17:26:00.619265Z","iopub.status.idle":"2022-03-31T17:26:00.624553Z","shell.execute_reply.started":"2022-03-31T17:26:00.619234Z","shell.execute_reply":"2022-03-31T17:26:00.623836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport torch.nn as nn\n\nfrom torch import optim\n\n\ntrain_accuracy=[]\nvalid_accuracy=[]\nepochs_total=[]\ndatadir = './data'\nmodel_dir = './ckpt'\nlogs = './log'\n\nworkers = 1\nbatchsize = 100\nsizeoffeatures, embeddingsize = 1024, 300\nnumofhidden, sizeofhiddenlayer = 3, 512\n\nepochs = 10\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#device='cuda'\nprint(device)\ndef train():\n    dataloader = data_loader(input_dir=datadir, batch_size=batchsize, num_worker=workers)\n    questionvocabularysize = qvocabsize+1\n    answervocabularysize = 3001\n    \n    model = VizWizModel(sizeoffeatures=sizeoffeatures, questionvocabularysize=questionvocabularysize, answervocabularysize=answervocabularysize,wordembeddingsize=embeddingsize, hidden_size=sizeofhiddenlayer, num_hidden=numofhidden).to(device)\n    \n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n   \n   \n    print('>> start training')\n    start_time = time.time()\n    for epoch in range(epochs):\n        metrics=[]\n        metrics1=[]\n        correct1=0\n        length1=0\n        correct=0\n        length=0\n        epoch_loss = {key: 0 for key in ['train', 'val']}\n        print(epoch)\n        model.train()\n        print(dataloader['train'])\n        for idx, dataentry in enumerate(dataloader['train']):\n            \n            image = dataentry['image'].to(device)\n            question = dataentry['question'].to(device)\n            label = dataentry['answer'].to(device)\n            answers=dataentry['answers']\n            label1=label.tolist()\n            \n            # forwardpropogation\n            logits = model(image, question)\n            \n            with torch.no_grad():\n                predict = torch.log_softmax(logits, dim=1)\n                predict = torch.argmax(predict, dim=1).tolist()\n            print(\"Train set prediction {}\".format(Counter(predict)))\n            for i in range(len(label1)):\n                if(predict[i]==label1[i]):\n                    correct+=1\n            length=length+batchsize\n            for i in range(answers.shape[0]):\n                count=0\n                for j in range(answers.shape[1]):\n                    if(answers[i][j]==predict[i]):\n                        count+=1\n                metrics.append(min(count//3,1))\n            loss = criterion(logits, label)\n            epoch_loss['train'] += loss.item()\n            # backpropogation\n            optimizer.zero_grad()\n            loss.backward()\n            print(loss.item())\n            optimizer.step()\n        model.eval()\n        correct = correct/length \n        print(\"Train set Accuracy is {}\".format(correct))\n        print(\"Train set Accuracy metric is {}\".format(np.mean(metrics)))\n        train_accuracy.append(np.mean(metrics))\n        \n        for idx, dataentry in enumerate(dataloader['val']):\n            image = dataentry['image'].to(device=device)\n            question = dataentry['question'].to(device=device)\n            label = dataentry['answer'].to(device=device)\n            answers=dataentry['answers']\n            label1=label\n            with torch.no_grad():\n                logits = model(image, question)\n                loss = criterion(logits, label)\n            epoch_loss['val'] += loss.item()\n            with torch.no_grad():\n                predict = torch.log_softmax(logits, dim=1)\n                predict = torch.argmax(predict, dim=1).tolist()\n            print(\"Valid set prediction {}\".format(Counter(predict)))\n            for i in range(len(predict)):\n                if(predict[i]==label1[i]):\n                    correct1+=1\n            length1=length1+batchsize\n            for i in range(answers.shape[0]):\n                count=0\n                for j in range(answers.shape[1]):\n                    if(answers[i][j]==predict[i]):\n                        count+=1\n                metrics1.append(min(count//3,1))\n        correct1 = correct1/length1\n        print(\"Validation set Accuracy is {}\".format(correct1))\n        print(\"Validation set Accuracy metric is {}\".format(np.mean(metrics1)))\n        valid_accuracy.append(np.mean(metrics1))\n        epochs_total.append(epoch)\n        # statistic\n        for phase in ['train', 'val']:\n            epoch_loss[phase] /= len(dataloader[phase])\n            with open(os.path.join(logs, f'{phase}_log.txt'), 'a') as f:\n                f.write(str(epoch+1) + '\\t' + str(epoch_loss[phase]) + '\\n')\n        print('Epoch:{}/{} | Training Loss: {train:6f} | Validation Loss: {val:6f}'.format(epoch+1, epochs, **epoch_loss))\n          \n        torch.save(model.state_dict(), os.path.join(model_dir, f'model-epoch-{epoch+1}.pth'))\n\n        scheduler.step()\n    end_time = time.time()\n    training_time = end_time - start_time\n    print(\"Total training time in mins is {}\".format(training_time//60))\n    \nif not os.path.exists(logs):\n    os.makedirs(logs)\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\ntrain()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:26:00.865563Z","iopub.execute_input":"2022-03-31T17:26:00.866187Z","iopub.status.idle":"2022-03-31T17:26:00.874103Z","shell.execute_reply.started":"2022-03-31T17:26:00.866137Z","shell.execute_reply":"2022-03-31T17:26:00.873209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# importing package\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# plot lines\nplt.plot(epochs_total, train_accuracy, label = \"train_accuracy\")\nplt.plot(epochs_total, valid_accuracy, label = \"validation_accuracy\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:26:01.324661Z","iopub.execute_input":"2022-03-31T17:26:01.325267Z","iopub.status.idle":"2022-03-31T17:26:01.32972Z","shell.execute_reply.started":"2022-03-31T17:26:01.325229Z","shell.execute_reply":"2022-03-31T17:26:01.328738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Final Model\n#Model\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.nn.functional as F\n\n\nclass ImgPreprocess(nn.Module):\n\n    def __init__(self, embed_dim):\n\n        super(ImgPreprocess, self).__init__()\n        self.model = models.resnet50(pretrained=True)\n        in_features = self.model.fc.in_features\n        self.model.fc =  nn.Linear(in_features, embed_dim)# remove resnet50 last layer\n      \n\n    def forward(self, image):\n        with torch.no_grad():\n            imagefeatures = self.model(image) # (batch, channel, height, width)\n            \n        l2_norm = F.normalize(imagefeatures, p=2, dim=1).detach()\n        return l2_norm\n\nclass QuestionPreprocess(nn.Module):\n    def __init__(self, questionvocabularysize, wordembeddingsize, hidden_size, num_hidden, sizeofconcatenatedlayer):\n                super(QuestionPreprocess, self).__init__()\n                self.word_embedding = nn.Embedding(questionvocabularysize, wordembeddingsize)\n                self.tanh = nn.Tanh()\n                self.hidden_size=hidden_size\n                self.num_hidden=num_hidden\n                self.lstm=nn.LSTM(wordembeddingsize,hidden_size,num_hidden,batch_first=True,bidirectional=True)\n                self.fc=nn.Linear(hidden_size*2,sizeofconcatenatedlayer)\n    def forward(self, question):\n                qu_embedding = self.word_embedding(question)  \n                qu_embedding = self.tanh(qu_embedding)\n                h0=torch.zeros(self.num_hidden*2,qu_embedding.size(0),self.hidden_size).to('cuda')\n                c0=torch.zeros(self.num_hidden*2,qu_embedding.size(0),self.hidden_size).to('cuda')\n                output,_=self.lstm(qu_embedding,(h0,c0))\n                output=self.fc(output[:,-1,:])\n                return output\nclass VizWizModel(nn.Module):\n\n    def __init__(self, sizeoffeatures, questionvocabularysize, answervocabularysize, wordembeddingsize, hidden_size, num_hidden):\n\n        super(VizWizModel, self).__init__()\n        self.questionpreprocess = QuestionPreprocess(questionvocabularysize, wordembeddingsize, hidden_size, num_hidden, sizeoffeatures)\n        self.imgpreprocess = ImgPreprocess(sizeoffeatures)\n        self.tanh = nn.Tanh()\n        self.dropout = nn.Dropout(0.5)\n        self.layer1 = nn.Linear(sizeoffeatures, answervocabularysize)\n        self.layer2 = nn.Linear(answervocabularysize, answervocabularysize)\n\n    def forward(self, image, question):\n\n        imagefeatures = self.imgpreprocess(image)               \n        queationfeatures = self.questionpreprocess(question)\n        dotproductfeatures = imagefeatures * queationfeatures\n        dotproductfeatures = self.dropout(dotproductfeatures)\n        dotproductfeatures = self.tanh(dotproductfeatures)\n        dotproductfeatures = self.layer1(dotproductfeatures)       \n        dotproductfeatures = self.dropout(dotproductfeatures)\n        dotproductfeatures = self.tanh(dotproductfeatures)\n        logits = self.layer2(dotproductfeatures)                 \n\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:24:15.171959Z","iopub.execute_input":"2022-03-31T18:24:15.172419Z","iopub.status.idle":"2022-03-31T18:24:15.197285Z","shell.execute_reply.started":"2022-03-31T18:24:15.172374Z","shell.execute_reply":"2022-03-31T18:24:15.196136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprecessed={}\npreprecessed['train']=preprocessingtrain('train')\npreprocessed['val']=preprocessing('val')\npreprecessed['train-val'] = preprecessed['train'] + preprecessed['val']","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:16:47.113787Z","iopub.execute_input":"2022-03-31T18:16:47.114309Z","iopub.status.idle":"2022-03-31T18:16:52.71637Z","shell.execute_reply.started":"2022-03-31T18:16:47.114272Z","shell.execute_reply":"2022-03-31T18:16:52.714367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" for key, value in preprecessed.items():\n        np.save(os.path.join(\"./data\", f'{key}.npy'), np.array(value))","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:25:20.96019Z","iopub.status.idle":"2022-03-31T17:25:20.960795Z","shell.execute_reply.started":"2022-03-31T17:25:20.960555Z","shell.execute_reply":"2022-03-31T17:25:20.960581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport torch.nn as nn\n\nfrom torch import optim\n\n\ntrain_accuracy=[]\nvalid_accuracy=[]\nepochs_total=[]\ndatadir = './data'\nmodel_dir = './ckpt'\nlogs = './log'\n\nworkers = 1\nbatchsize = 100\nsizeoffeatures, embeddingsize = 1024, 300\nnumofhidden, sizeofhiddenlayer = 3, 512\n\nepochs = 12\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#device='cuda'\nprint(device)\ndef train():\n    dataloader = data_loader(input_dir=datadir, batch_size=batchsize, num_worker=workers)\n    questionvocabularysize = qvocabsize+1\n    answervocabularysize = 3001\n    \n    model = VizWizModel(sizeoffeatures=sizeoffeatures, questionvocabularysize=questionvocabularysize, answervocabularysize=answervocabularysize,wordembeddingsize=embeddingsize, hidden_size=sizeofhiddenlayer, num_hidden=numofhidden).to(device)\n   \n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n   \n   \n    print('>> start training')\n    start_time = time.time()\n    for epoch in range(epochs):\n        metrics=[]\n        metrics1=[]\n        correct1=0\n        length1=0\n        correct=0\n        length=0\n        epoch_loss = {key: 0 for key in ['train', 'val']}\n        print(epoch)\n        model.train()\n        print(dataloader['train'])\n        for idx, dataentry in enumerate(dataloader['train']):\n            \n            image = dataentry['image'].to(device)\n            question = dataentry['question'].to(device)\n            label = dataentry['answer'].to(device)\n            answers=dataentry['answers']\n            label1=label.tolist()\n            \n            # forwardpropogation\n            logits = model(image, question)\n            \n            with torch.no_grad():\n                predict = torch.log_softmax(logits, dim=1)\n                predict = torch.argmax(predict, dim=1).tolist()\n            print(\"Train set prediction {}\".format(Counter(predict)))\n            for i in range(len(label1)):\n                if(predict[i]==label1[i]):\n                    correct+=1\n            length=length+batchsize\n            for i in range(answers.shape[0]):\n                count=0\n                for j in range(answers.shape[1]):\n                    if(answers[i][j]==predict[i]):\n                        count+=1\n                metrics.append(min(count//3,1))\n            loss = criterion(logits, label)\n            epoch_loss['train'] += loss.item()\n            # backpropogation\n            optimizer.zero_grad()\n            loss.backward()\n            print(loss.item())\n            optimizer.step()\n        model.eval()\n        correct = correct/length \n        print(\"Train set Accuracy is {}\".format(correct))\n        print(\"Train set Accuracy metric is {}\".format(np.mean(metrics)))\n        train_accuracy.append(np.mean(metrics))\n        \n        for idx, dataentry in enumerate(dataloader['val']):\n            image = dataentry['image'].to(device=device)\n            question = dataentry['question'].to(device=device)\n            label = dataentry['answer'].to(device=device)\n            answers=dataentry['answers']\n            label1=label\n            with torch.no_grad():\n                logits = model(image, question)\n                loss = criterion(logits, label)\n            epoch_loss['val'] += loss.item()\n            with torch.no_grad():\n                predict = torch.log_softmax(logits, dim=1)\n                predict = torch.argmax(predict, dim=1).tolist()\n            print(\"Valid set prediction {}\".format(Counter(predict)))\n            for i in range(len(predict)):\n                if(predict[i]==label1[i]):\n                    correct1+=1\n            length1=length1+batchsize\n            for i in range(answers.shape[0]):\n                count=0\n                for j in range(answers.shape[1]):\n                    if(answers[i][j]==predict[i]):\n                        count+=1\n                metrics1.append(min(count//3,1))\n        correct1 = correct1/length1\n        print(\"Validation set Accuracy is {}\".format(correct1))\n        print(\"Validation set Accuracy metric is {}\".format(np.mean(metrics1)))\n        valid_accuracy.append(np.mean(metrics1))\n        epochs_total.append(epoch)\n        # statistic\n        for phase in ['train', 'val']:\n            epoch_loss[phase] /= len(dataloader[phase])\n            with open(os.path.join(logs, f'{phase}_log.txt'), 'a') as f:\n                f.write(str(epoch+1) + '\\t' + str(epoch_loss[phase]) + '\\n')\n        print('Epoch:{}/{} | Training Loss: {train:6f} | Validation Loss: {val:6f}'.format(epoch+1, epochs, **epoch_loss))\n          \n        torch.save(model.state_dict(), os.path.join(model_dir, f'model-epoch-{epoch+1}.pth'))\n\n        scheduler.step()\n    end_time = time.time()\n    training_time = end_time - start_time\n    print(\"Total training time in mins is {}\".format(training_time//60))\n    \nif not os.path.exists(logs):\n    os.makedirs(logs)\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\ntrain()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:57:46.390263Z","iopub.execute_input":"2022-03-31T17:57:46.390546Z","iopub.status.idle":"2022-03-31T17:57:46.469196Z","shell.execute_reply.started":"2022-03-31T17:57:46.390514Z","shell.execute_reply":"2022-03-31T17:57:46.467961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing package\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# plot lines\nplt.plot(epochs_total, train_accuracy, label = \"train_accuracy\")\nplt.plot(epochs_total, valid_accuracy, label = \"validation_accuracy\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:25:20.963874Z","iopub.status.idle":"2022-03-31T17:25:20.96448Z","shell.execute_reply.started":"2022-03-31T17:25:20.964249Z","shell.execute_reply":"2022-03-31T17:25:20.964273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef preprocessingtest(type1):\n        missing_images = []\n\n   \n    ## load annotation file and question file\n        dataset = json.load(open(root_dir+'/'+type1+'.json'))\n        data = [None]*len(dataset)\n\n        for idx,one_data in enumerate(dataset):\n#             ans_counter = Counter([x['answer'] for x in one_data['answers']])\n#             allanswers = [x['answer'] for x in one_data['answers']]\n#             ans = ans_counter.most_common(1)[0][0]\n#             a_label = answer2answer_id.get(ans,3000)\n#             answers=[]\n#             if type1 == 'train' and a_label == -1:\n#                 continue\n#             for answer in allanswers :\n#                 answers.append(answer2answer_id.get(answer,3000))\n            \n       \n            imagename = one_data['image']\n            imagepath = \"./images_testresized\"+\"/\"+ imagename\n            questionsentence=one_data['question']\n            questiontokens = nltk.word_tokenize(question)\n            questionencoding = sentence_to_vocabid(one_data['question'], vocab)\n            if(len(questionencoding)>=37):\n                questionencoding=questionencoding[0:37]\n            else:\n                questionencoding.extend([0]*(37-len(questionencoding)))\n            image_info = {'imagename': imagename,\n                'imagepath': imagepath,\n                'questionsentence': questionsentence,\n                'questiontokens': questiontokens\n                }\n            \n            image_info['questionencoding'] = questionencoding\n            data[idx]=image_info\n        return data","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:16:59.485883Z","iopub.execute_input":"2022-03-31T18:16:59.486444Z","iopub.status.idle":"2022-03-31T18:16:59.497102Z","shell.execute_reply.started":"2022-03-31T18:16:59.486401Z","shell.execute_reply":"2022-03-31T18:16:59.496454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessed","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:57:56.059869Z","iopub.execute_input":"2022-03-31T17:57:56.060744Z","iopub.status.idle":"2022-03-31T17:57:56.077977Z","shell.execute_reply.started":"2022-03-31T17:57:56.060706Z","shell.execute_reply":"2022-03-31T17:57:56.076791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprecessed={}\npreprecessed['train']=preprocessingtrain('train')\npreprecessed['val']=preprocessingvalidation('val')\npreprecessed['test']=preprocessingtest('test')\npreprecessed['train-val'] = preprecessed['train'] + preprecessed['val']","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:17:02.133509Z","iopub.execute_input":"2022-03-31T18:17:02.134328Z","iopub.status.idle":"2022-03-31T18:17:11.565419Z","shell.execute_reply.started":"2022-03-31T18:17:02.134276Z","shell.execute_reply":"2022-03-31T18:17:11.56469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" for key, value in preprecessed.items():\n        np.save(os.path.join(\"./data\", f'{key}.npy'), np.array(value))","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:58:11.734702Z","iopub.execute_input":"2022-03-31T17:58:11.73503Z","iopub.status.idle":"2022-03-31T17:58:12.312366Z","shell.execute_reply.started":"2022-03-31T17:58:11.734988Z","shell.execute_reply":"2022-03-31T17:58:12.311638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ckpt_dir='../input/resizedviz/ckpt/model-epoch-12.pth'","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:58:13.264463Z","iopub.execute_input":"2022-03-31T17:58:13.264965Z","iopub.status.idle":"2022-03-31T17:58:13.269129Z","shell.execute_reply.started":"2022-03-31T17:58:13.264927Z","shell.execute_reply":"2022-03-31T17:58:13.267871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"workers = 1\nbatchsize = 100\nsizeoffeatures, embeddingsize = 1024, 300\nnumofhidden, sizeofhiddenlayer = 3, 512","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:58:15.486961Z","iopub.execute_input":"2022-03-31T17:58:15.487828Z","iopub.status.idle":"2022-03-31T17:58:15.492957Z","shell.execute_reply.started":"2022-03-31T17:58:15.48778Z","shell.execute_reply":"2022-03-31T17:58:15.492055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('./answer2answer_id.json', 'r') as f:\n    answer_data = json.load(f)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:58:15.996973Z","iopub.execute_input":"2022-03-31T17:58:15.997686Z","iopub.status.idle":"2022-03-31T17:58:16.003629Z","shell.execute_reply.started":"2022-03-31T17:58:15.997639Z","shell.execute_reply":"2022-03-31T17:58:16.00287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def swap(json):\n    ret = {}\n    for key in json:\n        ret[json[key]] = key\n  \n    return ret","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:58:16.316818Z","iopub.execute_input":"2022-03-31T17:58:16.317479Z","iopub.status.idle":"2022-03-31T17:58:16.322327Z","shell.execute_reply.started":"2022-03-31T17:58:16.317442Z","shell.execute_reply":"2022-03-31T17:58:16.320621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idtoanswer=swap(answer_data)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:58:17.053523Z","iopub.execute_input":"2022-03-31T17:58:17.054247Z","iopub.status.idle":"2022-03-31T17:58:17.059476Z","shell.execute_reply.started":"2022-03-31T17:58:17.054206Z","shell.execute_reply":"2022-03-31T17:58:17.058563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idtoanswer[0]","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:58:17.373035Z","iopub.execute_input":"2022-03-31T17:58:17.373763Z","iopub.status.idle":"2022-03-31T17:58:17.379026Z","shell.execute_reply.started":"2022-03-31T17:58:17.373713Z","shell.execute_reply":"2022-03-31T17:58:17.378181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questionvocabularysize = qvocabsize+1\nanswervocabularysize = 3001","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:58:17.821228Z","iopub.execute_input":"2022-03-31T17:58:17.821766Z","iopub.status.idle":"2022-03-31T17:58:17.826016Z","shell.execute_reply.started":"2022-03-31T17:58:17.821724Z","shell.execute_reply":"2022-03-31T17:58:17.824724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:58:19.006403Z","iopub.execute_input":"2022-03-31T17:58:19.007179Z","iopub.status.idle":"2022-03-31T17:58:19.010697Z","shell.execute_reply.started":"2022-03-31T17:58:19.007143Z","shell.execute_reply":"2022-03-31T17:58:19.009999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device='cuda'","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:58:19.277912Z","iopub.execute_input":"2022-03-31T17:58:19.278584Z","iopub.status.idle":"2022-03-31T17:58:19.281639Z","shell.execute_reply.started":"2022-03-31T17:58:19.278546Z","shell.execute_reply":"2022-03-31T17:58:19.280956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\npredictions=[]\ndef test(input_dir, data_type, batch_size, num_worker):\n    print(\"hi\")\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # convert to (C,H,W) and [0,1]\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # mean=0; std=1\n    ])\n    VizWiz_dataset = VizWizdataset(input_dir, f'{data_type}.npy', transformimage= transform)\n    dataloader = DataLoader(VizWiz_dataset, batch_size=batch_size, shuffle=False, num_workers=num_worker)\n   \n    model=VizWizModel(sizeoffeatures=sizeoffeatures, questionvocabularysize=questionvocabularysize, answervocabularysize=answervocabularysize,wordembeddingsize=embeddingsize, hidden_size=sizeofhiddenlayer, num_hidden=numofhidden).to(device)\n    model.load_state_dict(torch.load(ckpt_dir))\n    model.eval()\n    results = []\n    try:\n        for idx, dataentry in enumerate(dataloader):\n            image = dataentry['image'].to(device)\n            question = dataentry['question'].to(device)\n            imagepaths=dataentry['imagepath']\n            \n            if(idx==1500):\n                break\n            with torch.no_grad():\n               \n                logits = model(image, question)\n                predict = torch.log_softmax(logits, dim=1)\n                predict = torch.argmax(predict, dim=1).tolist()\n                \n                predict = [[idtoanswer[idx]] for idx in predict]\n                predict_final=predict\n                \n                for i in range(len(predict)):\n                     if(predict_final[i][0]=='<unk>'):\n                            predict_final[i][0]='unanswerable'\n                        \n                #imagepath=[[i] for i in imagepaths]\n                #pred=[[imagepath[i],predict[i]] for i in range(len(imagepath))]\n                predictions.extend(predict_final)\n        \n    except:\n        pass\n    file = open('results3.csv', 'w')\n    with file:    \n            write = csv.writer(file)\n            write.writerows(predictions)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:46:07.21721Z","iopub.execute_input":"2022-03-31T18:46:07.217502Z","iopub.status.idle":"2022-03-31T18:46:07.230874Z","shell.execute_reply.started":"2022-03-31T18:46:07.217465Z","shell.execute_reply":"2022-03-31T18:46:07.230132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test('./data', 'test', batch_size=100, num_worker=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T18:46:19.332366Z","iopub.execute_input":"2022-03-31T18:46:19.332646Z","iopub.status.idle":"2022-03-31T18:46:30.659575Z","shell.execute_reply.started":"2022-03-31T18:46:19.332616Z","shell.execute_reply":"2022-03-31T18:46:30.65857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}